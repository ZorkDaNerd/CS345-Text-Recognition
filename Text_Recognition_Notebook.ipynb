{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZorkDaNerd/CS345-Text-Recognition/blob/main/Text_Recognition_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "varJNv0ukj69"
      },
      "source": [
        "*This notebook is part of our text recognition project for class CS345 at Colorado State University.\n",
        "Original versions were created by Zachary Shimpa, Jenelle Dobyns and Jordan Rust.\n",
        "The content is availabe [on GitHub](github.com/ZorkDaNerd/CS345-Text-Recognition).*\n",
        "\n",
        "*Code help and referance was provided from Prof. Asa Ben-Hur and CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
        "Original versions of these notebooks were created by Asa Ben-Hur with updates by Ross Beveridge.\n",
        "The content is availabe [on his GitHub](https://github.com/asabenhur/CS345).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZorkDaNerd/CS345-Text-Recognition/blob/main/Text_Recognition_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuFviogMkj7C"
      },
      "source": [
        "Possible data sets\n",
        "\n",
        "https://github.com/amephraim/nlp/tree/master/code \n",
        "Dataset from this\n",
        "\n",
        "https://medium.com/mlearning-ai/sentiment-analysis-using-lstm-21767a130857\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sentiment_analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_HZZBhkj7C"
      },
      "source": [
        "# Description of Project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IlNExAU1o_xm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sl6KyJYkj7D"
      },
      "source": [
        "This project is about recognizing text emotions using LSTM. This is a form of natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_bAk2fPkj7D"
      },
      "source": [
        "### Coding languages and packages used in project\n",
        "\n",
        "Ex: Anaconda, Python, ect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KiYYTwEkj7E"
      },
      "outputs": [],
      "source": [
        "#Import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import our data. No train/test split yet - we have some data pre-processing to do!\n",
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    as_supervised=True)"
      ],
      "metadata": {
        "id": "PNvoDKw3p7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LniyqU7kqB2_",
        "outputId": "aba6fbe4-c23c-483f-af28-eca3705fd4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0])>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It's time to eliminate words and characters that don't contribute to the overall sentiment of our reviews\n",
        "# Let's import stopwords and parse our reviews to remove these unnecessary characters and words (like 'and', 'a', '!', etc.)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "def text_processing(text):\n",
        "    nopunc = []\n",
        "    for char in text:\n",
        "        if char not in string.punctuation:\n",
        "            if char!=str(\"0\") and char!=str(\"1\"):\n",
        "                nopunc.append(char)\n",
        "    nopunc = ''.join(nopunc)\n",
        "    \n",
        "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "TbjwDZWGq7XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIZr2uLxkj7F"
      },
      "source": [
        "# Project Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhGDDITekj7G"
      },
      "outputs": [],
      "source": [
        "#Defaults\n",
        "book = []\n",
        "thinBook = []\n",
        "uselessWords = [\"the\", \"and\", \"a\", \"to\", \"of\", \"i\", \"in\", \"a\", \"there\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpQWRRaVkj7G"
      },
      "source": [
        "This section puts all of the books into 2 numpy arrays, 1 fore the origional books and 1 for the thined version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-3CCyepkj7H"
      },
      "outputs": [],
      "source": [
        "for file in glob('texts\\*.txt'):\n",
        "    book.append(open(file, 'r').read())\n",
        "    thinBook.append(open(file, 'r').read())\n",
        "book = np.array(book)\n",
        "thinBook = np.array(thinBook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy81Dzkgkj7H"
      },
      "source": [
        "This section removes all of the unwanted words from the books and makes the words lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-BbYLt5kj7H"
      },
      "outputs": [],
      "source": [
        "for numb in range(len(thinBook)):\n",
        "    thinBook[numb] = thinBook[numb].translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    editBook = thinBook[numb].split()\n",
        "    thinedBook = [word for word in editBook if word not in uselessWords]\n",
        "    thinBook[numb] = ' '.join(thinedBook) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbdOjqW6kj7I"
      },
      "source": [
        "How many words and sentances are in the books? and how long would they take to read?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHjHgTLckj7I",
        "outputId": "06c1932c-7b23-4496-8445-25c166ae519d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Book 1 has 70804 words, 6760 sentences, and will take 6.0 hours to read.\n",
            "Book 2 has 78053 words, 7410 sentences, and will take 7.0 hours to read.\n",
            "Book 3 has 95546 words, 9409 sentences, and will take 8.0 hours to read.\n",
            "Book 4 has 196707 words, 18953 sentences, and will take 17.0 hours to read.\n"
          ]
        }
      ],
      "source": [
        "for numBook in range(len(book)):\n",
        "    wordCont = len(book[numBook].strip().split(\" \"))\n",
        "    sentCont = len(re.split(r'[.!?]+', book[numBook]))\n",
        "    timeRead = np.ceil((wordCont / 200)/60)\n",
        "    print(\"Book \" + str(numBook + 1) + \" has \" + str(wordCont) + \" words, \" + str(sentCont) + \" sentences, and will take \" + str(timeRead) + \" hours to read.\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b7RMnfjkj7J"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lOCpFikj7J"
      },
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFVuVafrkj7J"
      },
      "source": [
        "# Analysis of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joNdQkcNkj7J"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGlJktnpkj7J"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "171477609014cceea94aef133bb7309ada38ad52d4bc39cf4203d15bf533d055"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}