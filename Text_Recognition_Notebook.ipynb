{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This notebook is part of our text recognition project for class CS345 at Colorado State University.\n",
        "Original versions were created by Zachary Shimpa, Jenelle Dobyns and Jordan Rust.\n",
        "The content is availabe [on GitHub](github.com/ZorkDaNerd/CS345-Text-Recognition).*\n",
        "\n",
        "*Code help and referance was provided from Prof. Asa Ben-Hur and CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
        "Original versions of these notebooks were created by Asa Ben-Hur with updates by Ross Beveridge.\n",
        "The content is availabe [on his GitHub](https://github.com/asabenhur/CS345).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZorkDaNerd/CS345-Text-Recognition/blob/main/Text_Recognition_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Possible data sets\n",
        "\n",
        "\n",
        "https://www.kaggle.com/datasets/konradb/text-recognition-total-text-dataset\n",
        "\n",
        "\n",
        "https://github.com/topics/text-recognition\n",
        "\n",
        "https://analyticsindiamag.com/10-open-source-datasets-for-text-classification/\n",
        "\n",
        "https://github.com/amephraim/nlp/tree/master/code \n",
        "Dataset from this\n",
        "\n",
        "https://medium.com/mlearning-ai/sentiment-analysis-using-lstm-21767a130857\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sentiment_analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Description of Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This project is about recognizing text emotions using LSDM. This is a form of natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coding languages and packages used in project\n",
        "\n",
        "Ex: Anaconda, Python, ect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Defaults\n",
        "books = []\n",
        "thinBooks = []\n",
        "uselessWords = [\"the\", \"and\", \"a\", \"to\", \"of\", \"i\", \"in\", \"a\", \"there\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section puts all of the books into 2 numpy arrays, 1 fore the origional books and 1 for the thined version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Putting all of the books into a np array for convinance\n",
        "for file in glob('texts\\*.txt'):\n",
        "    books.append(open(file, 'r').read())\n",
        "    thinBooks.append(open(file, 'r').read())\n",
        "books = np.array(books)\n",
        "thinBooks = np.array(thinBooks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section removes all of the unwanted words from the books and makes the words lowercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "for numb in range(len(thinBooks)):\n",
        "    thinBooks[numb] = thinBooks[numb].translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    editBook = thinBooks[numb].split()\n",
        "    thinnedBook = [word for word in editBook if word not in uselessWords]\n",
        "    thinBooks[numb] = ' '.join(thinnedBook) \n",
        "#print(thinBooks[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyORst63L/8AIM6IicSpLrL2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "171477609014cceea94aef133bb7309ada38ad52d4bc39cf4203d15bf533d055"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
